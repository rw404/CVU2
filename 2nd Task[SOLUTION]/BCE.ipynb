{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0e9c59-00be-4e5b-a112-1db0f04b2e4c",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad26d40a-541c-4726-b797-25a06649ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be2a23-83c2-4c5e-a1b6-8de473775758",
   "metadata": {},
   "source": [
    "# Defined function\n",
    "\n",
    "Пусть мы хотим обучить сеть классифицировать числа. Рассмотрим функцию $f(x)=x>0$. Для простоты рассматривать будем числа на отрезке $[-5, 5] \\subset \\mathbb{R}$. \n",
    "\n",
    "- $x\\in[-5, 5]\\cap\\mathbb{R}$;\n",
    "- $f(x) = x>0$;\n",
    "- Функция ошибки $CrossEntropy(model) = -\\sum_{i=1}^{100}(i>0)\\log(model(i))$, где $model$ -- **однослойная нейронная сеть**; $model(i)$ -- **результат работы сети с входом i**;\n",
    "- Обучать будем 100 эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971893d8-35cf-492f-9718-d1feeb0782be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f887b2f2-bb02-4a98-9210-ba19224598a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "dif = 0\n",
    "for i in range(-5, 6):\n",
    "    dif += f(i)*2-1\n",
    "\n",
    "print(dif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95342e98-ceb2-464e-a1c7-4fbfca190004",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Для корректной работы сети, сперва нужно правильно сгруппировать данные, разделив их на 3 части:\n",
    "- Тестовая выборка - эту выборку алгоритм не использует в обучении, на ней будут показаны только итоговые значения точности\n",
    "- Тренировочная выборка - на этих данных сеть обучается:\n",
    "    - Тренировочная - на этих данных алгоритм обучает сеть\n",
    "    - Валидационная - на этих данных алгоритм после 1 итерации обучения оценивает показатели(затем может менять параметры сети для улучшения обучаемости)\n",
    "   \n",
    "Напишем функцию, производяющую деление выборки в соотношении fraction\\*len для обучения и (1-fraction)\\*len для тестирования.\n",
    "\n",
    "**P.s.** Самое главное, обучать сеть на равномерно распределенной выборке, для этого будем делить на описанные выше соотнешения элементы каждого класса(всего их 2), чтобы получить проавильно разбитые данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df3a4f8-4481-4254-8fcf-76c34fcbe733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(X, y, fraction=0.8):\n",
    "    size=X.shape[0]\n",
    "    per_class = int(0.5*fraction*size)\n",
    "    first_rand_idxs = np.random.choice(np.arange(size//2), per_class, replace=False)\n",
    "    second_rand_idxs = np.random.choice(np.arange(size//2, size), per_class, replace=False)\n",
    "    train_idxs = sorted([*first_rand_idxs, *second_rand_idxs])\n",
    "    test_idxs = np.array([i for i in range(size) if i not in train_idxs])\n",
    "    X_train, X_test = X[train_idxs], X[test_idxs]\n",
    "    y_train, y_test = y[train_idxs], y[test_idxs]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39c311e-bdf1-4f19-af26-06df06d0d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-5, 5, num=100)\n",
    "y = f(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af43f99-afb7-46b8-9949-666a4e0701cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = random_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4813fe6b-6908-4b27-bf3f-897037174cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80,)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43f8d4-75f9-41a1-ad38-25957a8950ad",
   "metadata": {},
   "source": [
    "## Validation & train\n",
    "\n",
    "Аналогично, разделим тренировочную выборку на тренировочную и валидационную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b26d1b8-023c-47ce-a269-ba86d789083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nn_train, X_nn_val, y_nn_train, y_nn_val = random_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eef292f-b34e-4476-bc10-0c7bf3b3f17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "print(X_nn_train.shape)\n",
    "print(X_nn_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124db284-9890-491e-a643-d50fab64785d",
   "metadata": {},
   "source": [
    "# Сама модель\n",
    "\n",
    "Рассмотрим в качестве модели линейный классификатор(1 нейрон) с одной постоянной смещения(в нашем случае она равна 0).\n",
    "\n",
    "$model(x) = x*w+b$\n",
    "\n",
    "Затем к выходу будет применяться нелинейный слой активации -- **сигмоида**: $sigmoid(x) = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56c0272a-0a99-46f1-af30-b2d4850d55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, eps=1e-5):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class Net:\n",
    "    def __init__(self, w=0.5, b=1):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return sigmoid(x*self.w+self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f800b5f4-32bf-43df-94f6-70a57b83a6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81757448]\n"
     ]
    }
   ],
   "source": [
    "net = Net(w=0.5, b=1)\n",
    "print(net(np.array([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5033178-1638-4b2a-88b1-34ced9266754",
   "metadata": {},
   "source": [
    "# Функция ошибки\n",
    "\n",
    "$L(x) = -\\frac{1}{16}\\sum_{i=1}^{16}(y\\_val[i]*\\log(model(X\\_nn\\_val[i]))+(1-y\\_val[i])*\\log(1-model(X\\_nn\\_val[i])))$\n",
    "\n",
    "Реализуем функцию ошибки для одиночного элемента, т.е. 1 слагаемое суммы сверху. Учтем, что из-за больших и малых чисел могут быть переполнения, поэтому в этих местах заменим значение на специально выбранное минимальное допустимое $e^{-5}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9dc2eb9-2819-437c-b314-9ea28f76f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCELoss(x, y, eps=1e-5):\n",
    "    new_x = np.where(x>eps, x, 10)\n",
    "    ans = np.zeros(x.shape)\n",
    "    ans[new_x>2] = 1\n",
    "    new_x = np.where(new_x<1-eps, new_x, -10)\n",
    "    ans[new_x>0] = -y[new_x>0]*np.log(new_x[new_x>0])-(1-y[new_x>0])*np.log(1-new_x[new_x>0])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d745e3e8-7276-4eb8-8622-404a1cc4c590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7200759760208356e-44\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(-100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0fb3602-af06-4c52-9a4f-5dae442e603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "print(BCELoss(sigmoid(np.array([100])), np.array([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7c77ba-1442-419f-9d36-b6223b1683a4",
   "metadata": {},
   "source": [
    "## Обратное распространение\n",
    "\n",
    "Уточним действие сети:\n",
    "\n",
    "1. Вход домножается на вес **w**: $r_1 = w\\times x$;\n",
    "2. К результату прибавляется константное смещение **b**: $l = w\\times x + b$;\n",
    "3. К полученным значениям применяется преобразование функцией-сигмоидой **r** = $sigmoid(l)$;\n",
    "4. Ошибка работы оценивается через **BCELoss**, сравнивая **r** с **y** -- известной меткой.\n",
    "\n",
    "Для обучения сети, как мы уже разбирали, мы должны идти от ошибки и постепенно обновлять веса от последнего слоя к первому.\n",
    "\n",
    "1. Оцениваем прозводную ошибки на выходе, т.к. решаем градиентным методом: $\\frac{\\partial L}{\\partial r} = -\\frac{\\partial \\left(y*\\log(r)+(1-y)*\\log(1-r)\\right)}{\\partial r} = -\\frac{y}{r} + \\frac{1-y}{1-r}$. Для ее вычисления мы знаем все значения;\n",
    "2. Теперь переходим к слою активации, т.е. к сигмоиде. Здесь нужно использовать преобразования: $\\frac{\\partial r}{\\partial l}$, где **r** полагаем равным значению сигмоиды от **l**, $\\frac{\\partial r}{\\partial l} = \\frac{\\partial \\frac{1}{1+e^{-l}}}{\\partial l} = \\frac{e^l(e^l+1)-e^{2l}}{(1+e^{l})^2} = \\frac{e^l}{(1+e^{l})^2} = \\frac{e^{-l}}{(1+e^{-l})^2} = \\frac{1}{1+e^{-l}}*\\left(1-\\frac{1}{1+e^{-l}}\\right) = r(1-r)$. Здесь мы получили значение производной через значение самой функции, поэтому сразу же можем ее вычислить. Далее пользуемся тем фактом, что $\\frac{\\partial L}{\\partial l} = \\frac{\\partial L}{\\partial r} * \\frac{\\partial r}{\\partial l}$;\n",
    "3. Осталось, наконец получить правильные значения коррекции настраеваемых параметров **w**, **b**: $\\frac{\\partial l}{\\partial w} = \\frac{\\partial (w\\times x + b)}{\\partial w} = x$; $\\frac{\\partial l}{\\partial b} = \\frac{\\partial (w\\times x + b)}{\\partial b} = 1$. Поэтому для **w** $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial l} * \\frac{\\partial l}{\\partial w} = \\frac{\\partial L}{\\partial r} * r(1-r)*x$; А для **b**: \n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial l} * \\frac{\\partial l}{\\partial b} = \\frac{\\partial L}{\\partial r} * r(1-r)$\n",
    "\n",
    "После того, как мы получили значения для производных каждого из весов, можем их менять, руководствуясь тем, о чем говорилось в теории(скорость обучения):\n",
    "\n",
    "$w = w - \\alpha*\\frac{\\partial L}{\\partial w}$\n",
    "\n",
    "$b = b - \\alpha*\\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "Теперь полученные функции занесем в метод обучения сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b01ea4c2-e5ad-4f53-b818-5b7c866b2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(net: Net, x, y, lr=0.01):\n",
    "    r = net(x)\n",
    "    #print(r)\n",
    "    divider_1 = np.where(r>1e-5, r, 1e-5)\n",
    "    divider_2 = np.where(1-r>1e-5, 1-r, 1e-5)\n",
    "    output_grad = -y/divider_1+(1-y)/divider_2\n",
    "    #print(output_grad)\n",
    "    #print((lr*output_grad*r*(1-r)*x).mean())\n",
    "    #print((lr*output_grad*r*(1-r)).mean())\n",
    "    net.w -= (lr*output_grad*r*(1-r)*x).mean()\n",
    "    net.b -= (lr*output_grad*r*(1-r)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce479a-8f68-4aa9-bfd1-136824007b49",
   "metadata": {},
   "source": [
    "# Сам процесс обучения\n",
    "\n",
    "У нас есть данные, на которых мы обучаем модель **X_nn_train**, на которых сравниваем качество в процессе обучения **X_nn_val**, наконец, те, на которых мы получим итоговое значение ошибки, заодно и точности **X_test**\n",
    "\n",
    "Есть сама модель **Net**, есть функция ошибки **BCELoss**, даже есть функция обновления весов для обучения **backprop**.\n",
    "\n",
    "Оставим исходными параметрами те, что заданы, обучим модель за 1000 итераций и посмотрим на точность каждой 100-ой итерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15346c5a-48ac-479f-a896-48e96f5b849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default quality: BCE=0.39386912930938783, Accuracy = 75.0%\n",
      "0.7267569161149006\n",
      "0.09981342050429531\n",
      "Epoch №0 -- quality: BCE=0.3884007833156735, Accuracy = 75.0%\n",
      "0.752084625318373\n",
      "0.09969512281747835\n",
      "Epoch №1 -- quality: BCE=0.38346623739566066, Accuracy = 75.0%\n",
      "0.7761399213102382\n",
      "0.0996380013210017\n",
      "Epoch №2 -- quality: BCE=0.37898619521843246, Accuracy = 75.0%\n",
      "0.7990550673077262\n",
      "0.09963594767842882\n",
      "Epoch №3 -- quality: BCE=0.37489668967777917, Accuracy = 75.0%\n",
      "0.8209426198002476\n",
      "0.0996836742335457\n",
      "Epoch №4 -- quality: BCE=0.37114552197370576, Accuracy = 75.0%\n",
      "0.8418991344210338\n",
      "0.09977657421415606\n",
      "Epoch №5 -- quality: BCE=0.3676896502619093, Accuracy = 75.0%\n",
      "0.8620080475197052\n",
      "0.09991061012725622\n",
      "Epoch №6 -- quality: BCE=0.36449324560368046, Accuracy = 75.0%\n",
      "0.8813419419175387\n",
      "0.10008222391783388\n",
      "Epoch №7 -- quality: BCE=0.3615262245971588, Accuracy = 75.0%\n",
      "0.8999643469392531\n",
      "0.10028826406656934\n",
      "Epoch №8 -- quality: BCE=0.35876312769719043, Accuracy = 75.0%\n",
      "0.9179311821918247\n",
      "0.1005259259792578\n",
      "Epoch №9 -- quality: BCE=0.35618225173968054, Accuracy = 75.0%\n",
      "Result quality: BCE=0.35618225173968054, Accuracy = 75.0%\n",
      "0.9179311821918247\n",
      "0.1005259259792578\n",
      "Test quality: BCE=0.35618225173968054, Accuracy = 75.0%\n"
     ]
    }
   ],
   "source": [
    "net = Net(w=0.7, b=0.1)\n",
    "\n",
    "#X_nn_train = X_nn_train.astype(np.int32)\n",
    "#y_nn_train = y_nn_train.astype(np.int32)\n",
    "#X_nn_val = X_nn_val.astype(np.int32)\n",
    "#y_nn_val = y_nn_val.astype(np.int32)\n",
    "X_train = X_train.astype(np.int32)\n",
    "y_train = y_train.astype(np.int32)\n",
    "X_test = X_test.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_nn_val = X_test\n",
    "y_nn_val = y_test\n",
    "X_nn_train = X_train\n",
    "y_nn_train = y_train\n",
    "\n",
    "print(f'Default quality: BCE={BCELoss(net(X_nn_val), y_nn_val).mean()}, Accuracy = {(net(X_nn_val).round()==y_nn_val).mean()*100}%')\n",
    "for it in range(10):\n",
    "    backprop(net, X_nn_train, y_nn_train, lr=0.1)\n",
    "    if it%1 == 0:\n",
    "        print(net.w)\n",
    "        print(net.b)\n",
    "        print(f'Epoch №{it} -- quality: BCE={BCELoss(net(X_nn_val), y_nn_val).mean()}, Accuracy = {(net(X_nn_val).round()==y_nn_val).mean()*100}%')\n",
    "\n",
    "print(f'Result quality: BCE={BCELoss(net(X_nn_val), y_nn_val).mean()}, Accuracy = {(net(X_nn_val).round()==y_nn_val).mean()*100}%')\n",
    "print(net.w)\n",
    "print(net.b)\n",
    "print(f'Test quality: BCE={BCELoss(net(X_test), y_test).mean()}, Accuracy = {(net(X_test).round()==y_test).mean()*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae8dc3-d33d-4e47-857c-81e620c27444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
